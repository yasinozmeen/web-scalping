import asyncio
import aiohttp
from bs4 import BeautifulSoup
import json
import os
from dotenv import load_dotenv
import argparse
from typing import Dict, Optional, Tuple, List
import random
import logging
from utils import setup_logger

# Logging yapÄ±landÄ±rmasÄ±
logger = setup_logger('scraper')

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# ScraperAPI anahtarÄ±nÄ± al
SCRAPER_API_KEY = os.getenv('SCRAPER_API_KEY')

async def random_sleep_async():
    """Asenkron random bekleme"""
    sleep_time = random.uniform(2, 5)
    logger.debug(f"ğŸ’¤ {sleep_time:.2f} saniye bekleniyor...")
    await asyncio.sleep(sleep_time)

async def scrape_url_async(url: str) -> Tuple[int, str]:
    """URL'yi asenkron olarak scrape et"""
    params = {
        'api_key': SCRAPER_API_KEY,
        'url': url,
        'country_code': 'us',
        'device_type': 'desktop',
        'render_js': '0',
        'timeout': '60000',
        'keep_headers': 'true',
        'premium': 'true'
    }
    
    try:
        logger.info(f"ğŸŒ URL scrape ediliyor: {url}")
        async with aiohttp.ClientSession() as session:
            async with session.get('http://api.scraperapi.com', params=params) as response:
                text = await response.text()
                logger.info(f"âœ… URL scrape edildi: {url}, Status={response.status}")
                return response.status, text
    except Exception as e:
        logger.error(f"âŒ Scraping hatasÄ±: URL={url}, Hata={str(e)}", exc_info=True)
        return 500, ""

async def get_variants_async(asin: str) -> List[str]:
    """ASIN'in varyasyonlarÄ±nÄ± asenkron olarak getir"""
    logger.info(f"ğŸ” ASIN {asin} iÃ§in varyasyonlar kontrol ediliyor...")
    url = f"https://www.amazon.com/dp/{asin}"
    
    try:
        status, html = await scrape_url_async(url)
        if status != 200:
            logger.error(f"âŒ Sayfa Ã§ekme hatasÄ±: Status={status}, ASIN={asin}")
            return [asin]
            
        soup = BeautifulSoup(html, 'lxml')
        variants = set()
        variants.add(asin)  # Mevcut ASIN'i ekle
        
        # Script iÃ§indeki varyasyonlarÄ± bul
        logger.debug("ğŸ” Script iÃ§inde varyasyonlar aranÄ±yor...")
        for script in soup.find_all('script'):
            text = script.string or ''
            if 'dimensionValuesDisplayData' in text:
                import re
                matches = re.findall(r'B[A-Z0-9]{9}', text)
                for match in matches:
                    variants.add(match)
        
        # Varyasyon butonlarÄ±ndan ASIN'leri topla
        logger.debug("ğŸ” Varyasyon butonlarÄ±nda ASIN'ler aranÄ±yor...")
        for element in soup.find_all(attrs={'data-defaultasin': True}):
            variant_asin = element.get('data-defaultasin')
            if variant_asin:
                variants.add(variant_asin)
        
        # Parent ASIN'i bul
        logger.debug("ğŸ” Parent ASIN aranÄ±yor...")
        parent_element = soup.find(attrs={'data-parent-asin': True})
        if parent_element:
            parent_asin = parent_element.get('data-parent-asin')
            if parent_asin:
                variants.add(parent_asin)
        
        variants = list(filter(lambda x: len(x) == 10 and x.startswith('B'), variants))
        logger.info(f"âœ… Toplam {len(variants)} varyasyon bulundu")
        if variants:
            logger.info("ğŸ“‹ Varyasyonlar:")
            for variant in variants:
                logger.info(f"   - {variant}")
                
        return list(variants)
        
    except Exception as e:
        logger.error(f"âŒ Sayfa iÅŸleme hatasÄ±: ASIN={asin}, Hata={str(e)}", exc_info=True)
        return [asin]

async def find_first_variant_position_async(keyword: str, variants: List[str]) -> Dict:
    """VaryasyonlarÄ±n pozisyonunu asenkron olarak bul"""
    logger.info(f"ğŸ” '{keyword}' aramasÄ±nda {len(variants)} varyasyon aranÄ±yor...")
    
    page_num = 1
    total_position = 0
    found_variant = None
    found_data = None
    
    while page_num <= 10:  # Ä°lk 10 sayfaya bakalÄ±m
        try:
            # Arama URL'ini oluÅŸtur
            if page_num == 1:
                url = f"https://www.amazon.com/s?k={keyword.replace(' ', '+')}"
            else:
                url = f"https://www.amazon.com/s?k={keyword.replace(' ', '+')}&page={page_num}"
            
            logger.info(f"ğŸ“„ Sayfa {page_num} kontrol ediliyor...")
            
            # SayfayÄ± scrape et
            status, html = await scrape_url_async(url)
            if status != 200:
                logger.error(f"âŒ Sayfa Ã§ekme hatasÄ±: Status={status}, Page={page_num}")
                page_num += 1
                continue
                
            soup = BeautifulSoup(html, 'lxml')
            products = []
            
            # ÃœrÃ¼nleri bul
            logger.debug(f"ğŸ” Sayfa {page_num}'de Ã¼rÃ¼nler aranÄ±yor...")
            for index, element in enumerate(soup.find_all(attrs={'data-asin': True}), 1):
                asin = element.get('data-asin')
                if asin:
                    sponsored = bool(element.find(attrs={'data-component-type': 'sp-sponsored-result'}))
                    products.append({
                        'asin': asin,
                        'position': index,
                        'sponsored': sponsored
                    })
            
            logger.info(f"ğŸ“Š Bu sayfada {len(products)} Ã¼rÃ¼n bulundu")
            
            for product in products:
                total_position += 1
                if product['asin'] in variants:
                    found_variant = product['asin']
                    found_data = {
                        'found': True,
                        'found_variant': product['asin'],
                        'page': page_num,
                        'page_position': product['position'],
                        'total_position': total_position,
                        'sponsored': product['sponsored']
                    }
                    logger.info(f"âœ… Varyasyon bulundu: {product['asin']}")
                    logger.info(f"ğŸ“Š Sayfa: {page_num}")
                    logger.info(f"ğŸ“Š Sayfa iÃ§i pozisyon: {product['position']}")
                    logger.info(f"ğŸ“Š Genel pozisyon: {total_position}")
                    logger.info(f"ğŸ·ï¸ Sponsorlu: {'Evet' if product['sponsored'] else 'HayÄ±r'}")
                    return found_data
            
            if found_variant:
                break
                
            page_num += 1
            await random_sleep_async()
            
        except Exception as e:
            logger.error(f"âŒ Sayfa {page_num} kontrol edilirken hata: {str(e)}", exc_info=True)
            page_num += 1
            continue
    
    logger.warning("âŒ HiÃ§bir varyasyon bulunamadÄ±!")
    return {
        'found': False,
        'found_variant': None,
        'page': None,
        'page_position': None,
        'total_position': None,
        'sponsored': None
    }

async def process_asin_async(asin: str, keyword: str) -> Dict:
    """ASIN'i asenkron olarak iÅŸle"""
    logger.info(f"ğŸ” Ä°ÅŸleniyor: ASIN={asin}, API Key={SCRAPER_API_KEY}")
    
    try:
        # VaryasyonlarÄ± bul
        variants = await get_variants_async(asin)
        
        if variants:
            # Ä°lk bulunan varyasyonun pozisyonunu bul
            result = await find_first_variant_position_async(keyword, variants)
            result['asin'] = asin
            result['keyword'] = keyword
            logger.info(f"âœ… Ä°ÅŸlem tamamlandÄ±: ASIN={asin}, SonuÃ§={result}")
            return result
            
    except Exception as e:
        logger.error(f"âŒ Genel hata: ASIN={asin}, Hata={str(e)}", exc_info=True)
    
    return {
        'asin': asin,
        'keyword': keyword,
        'found': False,
        'found_variant': None,
        'page': None,
        'page_position': None,
        'total_position': None,
        'sponsored': False
    }

async def scrape_product(asin: str, keyword: str) -> Dict:
    """
    Belirtilen ASIN ve anahtar kelime iÃ§in Amazon'da arama yapar.
    
    Args:
        asin (str): ÃœrÃ¼n ASIN'i
        keyword (str): Arama anahtar kelimesi
        
    Returns:
        Dict: Arama sonuÃ§larÄ±
    """
    try:
        logger.info(f"Scraping baÅŸlatÄ±lÄ±yor - ASIN: {asin}, Keyword: {keyword}")
        
        # ScraperAPI endpoint'i
        url = f"https://api.scraperapi.com"
        
        # API parametreleri
        params = {
            "api_key": SCRAPER_API_KEY,
            "url": f"https://www.amazon.com/s?k={keyword}",
            "render": "true",
            "keep_headers": "true",
            "premium": "true",
            "country_code": "us",
            "retry": "3"
        }
        
        async with aiohttp.ClientSession() as session:
            logger.debug("ScraperAPI isteÄŸi gÃ¶nderiliyor...")
            async with session.get(url, params=params, timeout=30) as response:
                if response.status != 200:
                    logger.error(f"ScraperAPI hatasÄ±: {response.status}")
                    return {
                        "asin": asin,
                        "keyword": keyword,
                        "found": False,
                        "found_variant": False,
                        "page": 0,
                        "page_position": 0,
                        "total_position": 0,
                        "sponsored": False
                    }
                
                html = await response.text()
                logger.debug("ScraperAPI yanÄ±tÄ± alÄ±ndÄ±")
                
                # HTML iÃ§eriÄŸini kontrol et
                if not html or len(html) < 1000:
                    logger.error("GeÃ§ersiz HTML yanÄ±tÄ±")
                    return {
                        "asin": asin,
                        "keyword": keyword,
                        "found": False,
                        "found_variant": False,
                        "page": 0,
                        "page_position": 0,
                        "total_position": 0,
                        "sponsored": False
                    }
                
                # SonuÃ§larÄ± iÅŸle
                result = {
                    "asin": asin,
                    "keyword": keyword,
                    "found": False,
                    "found_variant": False,
                    "page": 0,
                    "page_position": 0,
                    "total_position": 0,
                    "sponsored": False
                }
                
                # ASIN'i ara
                if asin in html:
                    logger.info(f"ASIN bulundu: {asin}")
                    result["found"] = True
                    # Sayfa ve pozisyon bilgilerini ekle
                    result["page"] = 1  # Ä°lk sayfada bulundu
                    result["page_position"] = 1  # Ä°lk pozisyonda bulundu
                    result["total_position"] = 1  # Toplam pozisyon
                else:
                    logger.info(f"ASIN bulunamadÄ±: {asin}")
                
                return result
                
    except asyncio.TimeoutError:
        logger.error("ScraperAPI zaman aÅŸÄ±mÄ±")
        return {
            "asin": asin,
            "keyword": keyword,
            "found": False,
            "found_variant": False,
            "page": 0,
            "page_position": 0,
            "total_position": 0,
            "sponsored": False
        }
    except Exception as e:
        logger.error(f"Beklenmeyen hata: {str(e)}")
        return {
            "asin": asin,
            "keyword": keyword,
            "found": False,
            "found_variant": False,
            "page": 0,
            "page_position": 0,
            "total_position": 0,
            "sponsored": False
        }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Amazon ASIN Scraper')
    parser.add_argument('-a', '--asin', required=True, help='Amazon ASIN')
    parser.add_argument('-k', '--keyword', required=True, help='Search keyword')
    args = parser.parse_args()
    
    result = asyncio.run(process_asin_async(args.asin, args.keyword))
    print(json.dumps(result, indent=2))
    
    # SonuÃ§larÄ± dosyaya kaydet
    with open('asin_results.json', 'w') as f:
        json.dump(result, f, indent=2)
